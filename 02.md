## 过拟合、欠拟合

### 1、过拟合、欠拟合的概念

#### 1.1 过拟合

在机器学习中，当一个统计模型首先描述随机误差或噪声，而不是自身的基本关系时，过度拟合就会出现。当一个模型是过于复杂，过拟合通常容易被发现，因为相对于训练数据类型的数量，参数的数量过于五花八门。那么这个模型由于过度拟合而效果不佳。举一个例子，在一个识别的任务当中，我们得到树叶的边缘是锯齿形的属性，这样在判断的过程中有锯齿形状属性会给树叶的识别增加一定的权重，当新的叶子没有锯齿形状的时候这个就很有可能被判断不是树叶，这样就导致了过度拟合。

​        从字面的意义上理解就是过度拟合的意思，常发生在线性分类器或者线性模型的训练和预测当中。过拟合的原理就是机器学习算法过度学习了训练集数据。

​        如果在针对训练集做曲线拟合的时候做得过于“完美”，那么当我们针对于其他预测集进行预测的时候，这套模型很有可能会失准，因为这套模型在训练的时候过度地接近于训练集的特征，缺乏鲁棒性；所以在机器学习训练过程中，100%的拟合训练集数据并不一定是好的。

#### 1.2 欠拟合

指我们训练的模型要求过于宽泛无法达到我们预期的效果正确率低表达能力差。

#### 1.3 交叉验证

​        有时亦称循环估计， 是一种统计学上将数据样本切割成较小子集的实用方法。于是可以先在一个子集上做分析， 而其它子集则用来做后续对此分析的确认及验证。 一开始的子集被称为训练集。而其它的子集则被称为验证集或测试集。交叉验证是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）。

​       交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　

​        交叉验证一般要尽量满足：

1）训练集的比例要足够多，一般大于一半 2）训练集和测试集要均匀抽样

交叉验证主要分成以下几类： 

**1）k-folder cross-validation:** k个子集，每个子集均做一次测试集，其余的作为训练集。交叉验证重复k次，每次选择一个子集作为测试集，并将k次的平均交叉验证识别正确率作为结果。 优点：所有的样本都被作为了训练集和测试集，每个样本都被验证一次。10-folder通常被使用。 

**2）K \* 2 folder cross-validation** 是k-folder cross-validation的一个变体，对每一个folder，都平均分成两个集合s0,s1，我们先在集合s0训练用s1测试，然后用s1训练s0测试。 优点是：测试和训练集都足够大，每一个个样本都被作为训练集和测试集。一般使用k=10 

**3)least-one-out cross-validation(loocv)** 假设dataset中有n个样本，那LOOCV也就是n-CV，意思是每个样本单独作为一次测试集，剩余n-1个样本则做为训练集。 优点：

1）每一回合中几乎所有的样本皆用于训练model，因此最接近母体样本的分布，估测所得的generalization error比较可靠。

2）实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。 但LOOCV的缺点则是计算成本高，为需要建立的models数量与总样本数量相同，当总样本数量相当多时，LOOCV在实作上便有困难，除非每次训练model的速度很快，或是可以用平行化计算减少计算所需的时间。

-------**十折交叉验证：10-fold cross validation**----------

​        英文名叫做10-fold cross-validation，用来测试算法准确性。是常用的测试方法。将数据集分成十分，轮流将其中9份作为训练数据，1份作为测试数据，进行试验。每次试验都会得出相应的正确率（或差错率）。10次的结果的正确率（或差错率）的平均值作为对算法精度的估计，一般还需要进行多次10折交叉验证（例如10次10折交叉验证），再求其均值，作为对算法准确性的估计。

​        之所以选择将数据集分为10份，是因为通过利用大量数据集、使用不同学习技术进行的大量试验，表明10折是获得最好误差估计的恰当选择，而且也有一些理论根据可以证明这一点。但这并非最终诊断，争议仍然存在。而且似乎5折或者20折与10折所得出的结果也相差无几。

### 2、权重衰减

**方法：**

​       权重衰减等价于$L_{2}$范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段之一。

**$L_{2}$范数正则化（regularization）：**

​        $L_{2}$范数正则化在模型原损失函数基础上添加$L_{2}$范数惩罚项，从而得到训练所需最小化的函数。

​        $L_{2}$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例
$$
l(w_1,w_2,b)=\frac{1}{n}\sum^{n}_{i=1}\frac{1}{2}\left(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)}\right)^2
$$
其中$w_1,w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)},x_2^{(i)}$，标签为$y^{(i)}$，样本为$n$。将权重参数用向量$\pmb{w}=[w_1,w_2]$表示，带有$L_{2}$范数惩罚项的新损失函数为
$$
l(w_1,w_2,b)+\frac{\lambda}{2n}|\pmb{w}|^2
$$
其中超参数$\lambda>0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重比较大，这通常会使学到的权重参数的元素较接近0.当$\lambda=0$时，惩罚项不起作用。上公式中$L_2$范数平方$|\pmb{w}|^2$展开后得到$w_1^2+w_1^2$。有了$L_2$范数惩罚项后，在小批量随机梯度下降中，将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为

![QQ截图20200218211233](C:\Users\Tinker\Desktop\QQ截图20200218211233.png)

### 3、丢弃法

多层感知机中神经网络图描述了一个单隐藏层的多层感知机。

丢弃法不改变输入的期望值。在测试模型时，我们为了拿到更加确定性的结果，一般不适用丢弃法。



## 梯度消失、梯度爆炸

在此补充学习和整理的梯度下降算法

**Gradient Descent-梯度下降法**

依据平方误差。定义该线性回归模型的损耗/代价函数（Cost Function）为： $$ J(\theta) = J(\theta_0, \theta_1,...,\theta_n) = \frac{1}{2m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^2 $$ 线性回归的损耗/代价函数的值与回归系数$\theta$的关系是碗状的。仅仅有一个最小点。线性回归的求解过程如同Logistic回归，差别在于学习模型函数$h_{\theta}(x)$不同。

深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。

当神经网络的层数较多时，模型的数值稳定性容易变差。

​        在神经网络中，通常需要随机初始化模型参数。

​        如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。



## 卷积神经网络（CNN）

**卷积神经网络（Convolutional Neural Network, CNN）**是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网络与普通神经网络非常相似，它们都由具有可学习的权重和偏置常量(biases)的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出是每个分类的分数，普通神经网络里的一些计算技巧到这里依旧适用。但它们的不同之处是卷积神经网络可以让我们把特定的性质编码入网络结构，使是前馈函数更加有效率，并减少了大量参数。

二维卷积层，常用于处理图像数据。

一个卷积神经网络由很多层组成，它们的输入是三维的，输出也是三维的，有的层有参数，有的层不需要参数。

卷积神经网络通常包含以下几种层：

**卷积层（Convolutional layer）**，卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。
**线性整流层（Rectified Linear Units layer, ReLU layer）**，这一层神经的活性化函数（Activation function）使用线性整流（Rectified Linear Units, ReLU）f(x)=max(0,x)f(x)=max(0,x)。
**池化层（Pooling layer）**，通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。
**全连接层（ Fully-Connected layer）**, 把所有局部特征结合变成全局特征，用来计算最后每一类的得分。

**卷积神经网络进阶**

深度卷积神经网络，超越手工涉及的特征。

**特征：**

1. 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
2. 将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. 用Dropout来控制全连接层的模型复杂度。
4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

**程序过程：**

设计基本参数和框架-->载入数据集-->训练

还有一个重要的概念：使用重复元素的网络（CGG）

VGG：通过重复使⽤简单的基础块来构建深度模型。
Block:数个相同的填充为1、窗口形状为3×33×3的卷积层,接上一个步幅为2、窗口形状为2×22×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。